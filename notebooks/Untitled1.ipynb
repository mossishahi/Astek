{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:46.826289Z",
     "iopub.status.busy": "2021-07-05T08:49:46.825809Z",
     "iopub.status.idle": "2021-07-05T08:49:47.861410Z",
     "shell.execute_reply": "2021-07-05T08:49:47.860345Z",
     "shell.execute_reply.started": "2021-07-05T08:49:46.826168Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os, glob\n",
    "import logging\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:49.047905Z",
     "iopub.status.busy": "2021-07-05T08:49:49.047544Z",
     "iopub.status.idle": "2021-07-05T08:49:50.991289Z",
     "shell.execute_reply": "2021-07-05T08:49:50.990083Z",
     "shell.execute_reply.started": "2021-07-05T08:49:49.047860Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:50.992821Z",
     "iopub.status.busy": "2021-07-05T08:49:50.992573Z",
     "iopub.status.idle": "2021-07-05T08:49:51.202912Z",
     "shell.execute_reply": "2021-07-05T08:49:51.201829Z",
     "shell.execute_reply.started": "2021-07-05T08:49:50.992788Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:51.204696Z",
     "iopub.status.busy": "2021-07-05T08:49:51.204444Z",
     "iopub.status.idle": "2021-07-05T08:49:51.213171Z",
     "shell.execute_reply": "2021-07-05T08:49:51.212372Z",
     "shell.execute_reply.started": "2021-07-05T08:49:51.204662Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:52.651426Z",
     "iopub.status.busy": "2021-07-05T08:49:52.651034Z",
     "iopub.status.idle": "2021-07-05T08:49:52.662059Z",
     "shell.execute_reply": "2021-07-05T08:49:52.660896Z",
     "shell.execute_reply.started": "2021-07-05T08:49:52.651369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/m.shah/projects/models/kaggle-models'"
      ]
     },
     "execution_count": 679,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='/home/m.shah/projects/models/kaggle-models/qapp.log', \n",
    "                    filemode='a', format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T08:49:54.025381Z",
     "iopub.status.busy": "2021-07-05T08:49:54.024994Z",
     "iopub.status.idle": "2021-07-05T08:49:55.201321Z",
     "shell.execute_reply": "2021-07-05T08:49:55.200124Z",
     "shell.execute_reply.started": "2021-07-05T08:49:54.025334Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(path, \"../../data/simulated-data-raw/\", \"data\", \"*.pkl\")):\n",
    "    with open(filename, 'rb') as f:\n",
    "        temp = pd.read_pickle(f)\n",
    "        dfs.append(temp)\n",
    "df = pd.DataFrame()\n",
    "df = df.append(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-04T12:48:17.948064Z",
     "iopub.status.busy": "2021-07-04T12:48:17.947787Z",
     "iopub.status.idle": "2021-07-04T12:48:18.265623Z",
     "shell.execute_reply": "2021-07-04T12:48:18.264282Z",
     "shell.execute_reply.started": "2021-07-04T12:48:17.948030Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.iloc[:, [0, 1, 2, 3, 4, 5, 6, 8, 7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T09:02:28.342349Z",
     "iopub.status.busy": "2021-07-05T09:02:28.341949Z",
     "iopub.status.idle": "2021-07-05T09:02:42.281797Z",
     "shell.execute_reply": "2021-07-05T09:02:42.280924Z",
     "shell.execute_reply.started": "2021-07-05T09:02:28.342304Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feature Define\n",
    "if \"WEEK_DAY\" not in df.columns:\n",
    "    df.insert(7, \"WEEK_DAY\", df[\"TX_DATETIME\"].apply(lambda x : x.weekday()))\n",
    "\n",
    "#Feature Selection\n",
    "selected_features = [\"CUSTOMER_ID\",\n",
    "                     \"TERMINAL_ID\",\n",
    "                     'TX_AMOUNT', \n",
    "                     'TX_TIME_SECONDS', \n",
    "                     'TX_TIME_DAYS', \n",
    "                     \"WEEK_DAY\", \n",
    "                     \"TX_FRAUD_SCENARIO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T09:14:56.029136Z",
     "iopub.status.busy": "2021-07-05T09:14:56.028726Z",
     "iopub.status.idle": "2021-07-05T09:14:56.072025Z",
     "shell.execute_reply": "2021-07-05T09:14:56.070736Z",
     "shell.execute_reply.started": "2021-07-05T09:14:56.029089Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:    \n",
    "    def __init__(self, input_data, logger):\n",
    "        self.logger = logger\n",
    "        self.data = input_data\n",
    "    def pre_process(self, \n",
    "                    feature_columns,\n",
    "                    label_columns,\n",
    "                    window_size = 128,\n",
    "                    numericals = None,\n",
    "                    categoricals = None,\n",
    "                    test_train_split = 0.7,\n",
    "                    roll_base = 'time',\n",
    "                    drop_rollbase = True,\n",
    "                    imbalanced = False):\n",
    "        # ---------- \n",
    "        self.features = feature_columns\n",
    "        self.labels = label_columns\n",
    "        self.window_size = window_size\n",
    "        self.drop_rollbase = drop_rollbase\n",
    "        self.test_train_split = test_train_split\n",
    "        if 'TX_DATETIME' in feature_columns:\n",
    "            self.data['TX_DATETIME'] = self.data['TX_DATETIME'].values.astype(float) \n",
    "        # -----------\n",
    "        self.roll_base = roll_base\n",
    "        self.customers = np.array([])\n",
    "        # ---------- Standardization\n",
    "        self.scaler(numericals)\n",
    "        # ---------- Categorical features, OneHotEncoding\n",
    "        if categoricals is not None:\n",
    "            self.logger.info(str((\"Raw data shape:\", self.data.shape)))\n",
    "            self.encode(categoricals)\n",
    "            self.logger.info(str((\"categorized data shape:\", self.data.shape)))\n",
    "        # ---------- Rolling X and Y Tensors\n",
    "        self.logger.info( \"======== Making X and Y Tensors ==========\")\n",
    "        self.x_tensor, self.y_tensor = self.roll(self.window_size)\n",
    "        self.logger.info( \"Input X by shape:\", self.data.shape,\"is rolled to X Tensor: \", self.x_tensor.shape)\n",
    "        self.logger.info( \"X and Y tensors are Rolled \\n\")\n",
    "\n",
    "        self.train_idx = np.random.choice(self.x_tensor.shape[0], \n",
    "                                      int(self.x_tensor.shape[0]*self.test_train_split), replace=False)                      \n",
    "        self.X_train = self.x_tensor[self.train_idx, :, :]\n",
    "        self.X_test = np.delete(self.x_tensor, self.train_idx, axis=0)\n",
    "        self.y_train = self.y_tensor[self.train_idx, ]\n",
    "        self.y_test = np.delete(self.y_tensor, self.train_idx, axis=0)\n",
    "        self.logger.info( \"Preprocessing Done! \\n\")\n",
    "        return self.X_train, self.X_test, self.y_train, self.y_test\n",
    "                \n",
    "    def encode(self, categoricals):\n",
    "        if categoricals[0] is not None:\n",
    "            self.data = pd.get_dummies(self.data, columns = categoricals[0])\n",
    "        if categoricals[1] is not None:\n",
    "            self.data = pd.get_dummies(self.data, columns = categoricals[1])\n",
    "    \n",
    "    def scaler(self, numericals):\n",
    "        scaler = MinMaxScaler()\n",
    "        self.data[numericals[0]] = scaler.fit_transform(self.data[numericals[0]])\n",
    "        self.data[numericals[1]] = scaler.fit_transform(self.data[numericals[1]])\n",
    "\n",
    "    def roll(self, window_size):\n",
    "        x_filter = [col for col in self.data.columns if col.startswith(tuple(self.features))]\n",
    "        y_filter = [col for col in self.data.columns if col.startswith(tuple(self.labels))]\n",
    "        ix_tensor = np.zeros([(self.data.shape[0] - (window_size)) * window_size, len(x_filter)], dtype = 'float32')\n",
    "        iy_tensor = np.zeros((0, len(y_filter)), dtype = 'float32')\n",
    "\n",
    "        if self.roll_base == 'time':\n",
    "            self.dg_x = self.data[x_filter]\n",
    "            self.dg_y = self.data[y_filter]\n",
    "            # - - - - - Check Drive files - - - - \n",
    "            if os.path.isfile('ix_tensor.pickle'):\n",
    "                print(\"- - - -  X Tensor founded on Local Drive - - - - \")\n",
    "                print(\"- - - -  Reading X TENSOR - - - - \")\n",
    "                with open('ix_tensor.pickle', 'rb') as file:\n",
    "                    ix_tensor = joblib.load(file)\n",
    "                with open('iy_tensor.pickle', 'rb') as file:\n",
    "                    iy_tensor = joblib.load(file)\n",
    "            else:\n",
    "                # Rolling Loop for making ix_tensor\n",
    "                for i in tqdm(range(self.dg_x.shape[0]-(window_size))):\n",
    "                    s = np.array(self.dg_x[i:i+window_size], dtype='float32')\n",
    "                    ix_tensor[(window_size*i):(window_size*(i+1)), :] = s\n",
    "                    iy_tensor = np.vstack((iy_tensor, self.dg_y.iloc[i+window_size, ]))\n",
    "                ix_tensor = ix_tensor.reshape(-1, window_size, np.shape(ix_tensor)[1])\n",
    "                self.logger.info( \"- - - - Writing X Tensor on Drive- - - -\")\n",
    "                with open('ix_tensor.pickle', 'wb') as file:\n",
    "                    joblib.dump(ix_tensor, file)\n",
    "                with open('iy_tensor.pickle', 'wb') as file:\n",
    "                    joblib.dump(iy_tensor, file) \n",
    "        else:\n",
    "            if os.path.isfile('ix_tensor.pickle'):\n",
    "                print(\"- - - -  X Tensor founded on Local Drive - - - - \")\n",
    "                print(\"- - - -  Reading X TENSOR - - - - \")\n",
    "                with open('ix_tensor.pickle', 'rb') as file:\n",
    "                    ix_tensor = joblib.load(file)\n",
    "                with open('iy_tensor.pickle', 'rb') as file:\n",
    "                    iy_tensor = joblib.load(file)\n",
    "            else:\n",
    "                dg = self.data.sort_values(by = self.roll_base)\n",
    "                self.dg_x = dg[x_filter]\n",
    "                self.dg_y = dg[y_filter]\n",
    "                print(self.dg_x.columns)\n",
    "                del(dg)\n",
    "                # ------- Rolling Looop -----\n",
    "                i = 0\n",
    "                j = 0\n",
    "                while (i < self.dg_x.shape[0]-(window_size)):\n",
    "                    if(self.dg_x[self.roll_base[0]].iloc[i + window_size] == self.dg_x[self.roll_base[0]].iloc[i + window_size - 1]):\n",
    "                        s = np.array(self.dg_x[i:i + window_size], dtype='float32')\n",
    "                        ix_tensor[(window_size*j):(window_size*(j+1)), :] = s\n",
    "                        iy_tensor = np.vstack((iy_tensor, self.dg_y.iloc[i+window_size,:]))\n",
    "                        self.customers = np.append(self.customers, self.dg_x[self.roll_base[0]][i + window_size])\n",
    "                        j += 1\n",
    "                        i += 1\n",
    "                    else:\n",
    "                        i += window_size\n",
    "                        continue\n",
    "                # -------- Drop RoleBase Column -----\n",
    "                if self.drop_rollbase:\n",
    "                    ix_tensor = np.delete(ix_tensor, self.dg_x.columns.get_loc(self.roll_base[0]), axis = 1)\n",
    "                ix_tensor = ix_tensor[:len(iy_tensor)*window_size ,:].reshape(-1, window_size, ix_tensor.shape[1])\n",
    "                iy_tensor = iy_tensor.reshape(-1, 1)\n",
    "                # ------- Dump Tensors on Drive ------\n",
    "                with open('ix_tensor.pickle', 'wb') as file:\n",
    "                    joblib.dump(ix_tensor, file)\n",
    "                with open('iy_tensor.pickle', 'wb') as file:\n",
    "                    joblib.dump(iy_tensor, file) \n",
    "        return ix_tensor, iy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-01T08:08:59.998603Z",
     "iopub.status.busy": "2021-07-01T08:08:59.998158Z",
     "iopub.status.idle": "2021-07-01T08:09:00.013030Z",
     "shell.execute_reply": "2021-07-01T08:09:00.011842Z",
     "shell.execute_reply.started": "2021-07-01T08:08:59.998549Z"
    }
   },
   "outputs": [],
   "source": [
    "class LSTM_REGRESSION:\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 lstm_units = 50,\n",
    "                 n_outputs = 1,\n",
    "                 optimizer = 'adam',\n",
    "                loss = 'mse',\n",
    "                metrics = 'mae'):\n",
    "        #Model Layers\n",
    "        self.model = Sequential()\n",
    "        self.model.add(LSTM(lstm_units, input_shape=input_shape, return_sequences = True))#, return_sequences=True))            \n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(lstm_units, return_sequences = True))            \n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(LSTM(lstm_units))            \n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(n_outputs))\n",
    "        \n",
    "        self.model.compile(loss=loss, \n",
    "                           optimizer=optimizer, \n",
    "                           metrics=metrics) #optimizer='rmsprop'\n",
    "        self.model.build()\n",
    "        print(\"Model Summary: \\n\\n\", self.model.summary())\n",
    "        self.model\n",
    "        \n",
    "    def train(self, train_x, train_y, batch_size = 100, epochs  =1):\n",
    "        csv_logger = CSVLogger('/home/m.shah/projects/models/kaggle-models/training.log', append=True, separator=';')\n",
    "        result = self.model.fit(train_x,\n",
    "                                train_y, \n",
    "                                batch_size = batch_size,\n",
    "                                validation_split = 0.2,\n",
    "                                epochs = epochs,\n",
    "                               callbacks=[csv_logger])\n",
    "        return result\n",
    "    def predict(self, test_x):\n",
    "        return self.model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T09:14:59.391989Z",
     "iopub.status.busy": "2021-07-05T09:14:59.391579Z",
     "iopub.status.idle": "2021-07-05T09:14:59.397877Z",
     "shell.execute_reply": "2021-07-05T09:14:59.396357Z",
     "shell.execute_reply.started": "2021-07-05T09:14:59.391944Z"
    }
   },
   "outputs": [],
   "source": [
    "pp = Preprocessor(df.iloc[:50000, :], logging.getLogger('pre-processor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-05T09:15:00.850642Z",
     "iopub.status.busy": "2021-07-05T09:15:00.850222Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-867-fb6913b8ec43>:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[numericals[0]] = scaler.fit_transform(self.data[numericals[0]])\n",
      "/home/m.shah/.conda/envs/tensorflow/lib/python3.8/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n",
      "<ipython-input-867-fb6913b8ec43>:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.data[numericals[1]] = scaler.fit_transform(self.data[numericals[1]])\n",
      "/home/m.shah/.conda/envs/tensorflow/lib/python3.8/site-packages/pandas/core/indexing.py:1738: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value[:, i].tolist(), pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - - -  X Tensor founded on Local Drive - - - - \n",
      "- - - -  Reading X TENSOR - - - - \n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y, test_y = pp.pre_process(selected_features, \n",
    "                               ['TX_AMOUNT'], \n",
    "                               numericals = [[\"TX_AMOUNT\", \"TX_TIME_SECONDS\",'TX_TIME_DAYS'],[\"TX_AMOUNT\"]],\n",
    "                               categoricals = [[\"TERMINAL_ID\", \"WEEK_DAY\", \"TX_FRAUD_SCENARIO\"],None], \n",
    "                               window_size = 8,\n",
    "                               roll_base = [\"CUSTOMER_ID\", \"TX_TIME_SECONDS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 8, 8)              317120    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 8, 8)              544       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8)              0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 318,217\n",
      "Trainable params: 318,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model Summary: \n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_REGRESSION(train_x.shape[1:], n_outputs = train_y.shape[1], lstm_units = train_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "104/104 [==============================] - 30s 180ms/step - loss: 0.0129 - mae: 0.0601 - val_loss: 0.0097 - val_mae: 0.0511\n",
      "Epoch 2/2\n",
      "104/104 [==============================] - 15s 143ms/step - loss: 0.0089 - mae: 0.0515 - val_loss: 0.0093 - val_mae: 0.0618\n"
     ]
    }
   ],
   "source": [
    "history = model.train(train_x, train_y, batch_size = 128, epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 913,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_history.pickle', 'wb') as file:\n",
    "    joblib.dump(history.history, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
